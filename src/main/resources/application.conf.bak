evalscope {
  # Model configurations
  models {
    mock-chat-model {
      type = "chat"
      provider = "mock"
      enabled = false
      parameters {
        endpoint = "mock://localhost:8080"
        max_tokens = 100
        temperature = 0.7
      }
    }

    # Example configuration for OpenAI GPT model
    gpt-3.5-turbo {
      type = "chat"
      provider = "openai"
      enabled = false
      parameters {
        model_name = "gpt-3.5-turbo"
        max_tokens = 1000
        temperature = 0.7
        top_p = 1.0
      }
      credentials {
        api_key = "${OPENAI_API_KEY}"
      }
    }

    # Example configuration for embedding model
    text-embedding-ada-002 {
      type = "embedding"
      provider = "openai"
      enabled = false
      parameters {
        model_name = "text-embedding-ada-002"
        dimensions = 1536
      }
      credentials {
        api_key = "${OPENAI_API_KEY}"
      }
    }

    # deepseek-v3-0324
    deepseek-v3-0324 {
      type = "embedding"
      provider = "custom"
      enabled = true
      parameters {
        model_name = "deepseek-v3-0324"
        dimensions = 1024
        endpoint = "http://10.100.42.231/test/v1/chat/completions"
      }
      credentials {
        api_key = "e40cb079-9214-43a8-9355-6cca521e8421"
      }
    }

  }

  # Evaluation configurations
  evaluations {
    default_evaluation {
      models = ["mock-chat-model"]
      evaluators = ["chat", "performance"]
      maxConcurrency = 1
      saveResults = true
      outputPath = "results"
      parameters {
        max_examples = 20
        timeout_seconds = 30
      }
    }

    comprehensive_evaluation {
      models = ["mock-chat-model"]
      evaluators = ["chat", "performance"]
      maxConcurrency = 2
      saveResults = true
      outputPath = "results/comprehensive"
      parameters {
        max_examples = 100
        timeout_seconds = 60
        warmup_iterations = 10
        test_iterations = 200
      }
    }

    perf_model {
      models = ["deepseek-v3-0324"]}
      evaluators = ["performance"]
      maxConcurrency = 3
      saveResults = true
      outputPath = "results/performance"
      parameters {
        max_examples = 50
        timeout_seconds = 20
        warmup_iterations = 5
        test_iterations = 100
      }
  }

  # Global settings
  settings {
    max_job_concurrency = 5
    response_timeout_seconds = 30
    result_format = "json"  # json, csv, xml
    log_level = "INFO"
  }
}