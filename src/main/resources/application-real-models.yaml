# 真实AI模型评估配置文件
# 这个配置文件演示如何连接和评估真实的AI模型
# 基于 https://github.com/modelscope/evalscope 的设计思想

evalscope:
  models:
    # === OpenAI GPT 模型 ===
    openai-gpt-3.5-turbo:
      type: "chat"
      provider: "openai"
      enabled: true
      parameters:
        # API基础配置（必填）
        endpoint: "https://api.openai.com/v1"
        model_name: "gpt-3.5-turbo"

        # 模型行为参数
        max_tokens: 2048
        temperature: 0.7
        top_p: 0.9
        frequency_penalty: 0.0
        presence_penalty: 0.0

        # 连接配置
        connect_timeout: 30          # 连接超时（秒）
        read_timeout: 60             # 读取超时（秒）
        max_retries: 3               # 最大重试次数
        retry_delay: 1000            # 重试间隔（毫秒）

        # 请求速率限制
        max_workers: 10              # 最大并发连接数

      credentials:
        api_key: "${OPENAI_API_KEY}"  # 必须从环境变量读取API密钥

    # === OpenAI GPT-4 模型 ===
    openai-gpt-4:
      type: "chat"
      provider: "openai"
      enabled: false         # 默认禁用比较昂贵的模型
      parameters:
        endpoint: "https://api.openai.com/v1"
        model_name: "gpt-4"
        max_tokens: 4096
        temperature: 0.8
        top_p: 0.95
        connect_timeout: 30
        read_timeout: 90        # GPT-4可能更慢，需要更长的读取超时
        max_retries: 3
      credentials:
        api_key: "${OPENAI_API_KEY}"

    # === HuggingFace Hub 模型 ===
    huggingface-microsoft-dialogpt-medium:
      type: "chat"
      provider: "huggingface"
      enabled: true
      parameters:
        model_name: "microsoft/DialoGPT-medium"
        endpoint: "https://api-inference.huggingface.co"

        # HuggingFace 参数
        max_tokens: 512
        temperature: 0.7
        top_p: 0.9
        top_k: 50

        # HF模型可能需要更长的加载时间
        read_timeout: 120
        connect_timeout: 30
        max_retries: 5
        retry_delay: 2000

      credentials:
        api_token: "${HF_API_TOKEN}"  # 可选，某些模型可能需要HF token

    # === HuggingFace 其他模型示例 ===
    huggingface-distilgpt2:
      type: "text_generation"
      provider: "huggingface"
      enabled: false
      parameters:
        model_name: "distilgpt2"
        endpoint: "https://api-inference.huggingface.co"
        max_tokens: 256
        temperature: 0.8
        read_timeout: 60
      credentials:
        api_token: "${HF_API_TOKEN}"

    # === 本地部署模型（兼容OpenAI格式）===
    local-llama2-7b-chat:
      type: "chat"
      provider: "local"
      enabled: false     # 需要本地服务运行
      parameters:
        endpoint: "http://localhost:8000/v1"
        model_name: "llama-2-7b-chat"
        max_tokens: 1024
        temperature: 0.8

        # 本地服务通常更快，可以设置较短超时
        connect_timeout: 10
        read_timeout: 30
        max_retries: 2
      credentials:
        api_key: "local-key"   # 有些本地服务不需要认证

    # === Azure OpenAI 服务 ===
    azure-openai-gpt-35:
      type: "chat"
      provider: "azure-openai"
      enabled: false
      parameters:
        # Azure OpenAI 端点格式通常包含部署名称
        endpoint: "https://your-resource.openai.azure.com/openai/deployments/gpt-35-turbo"
        model_name: "gpt-35-turbo"
        api_version: "2023-05-15"

        max_tokens: 2048
        temperature: 0.7
        connect_timeout: 30
        read_timeout: 60
        max_retries: 3
      credentials:
        api_key: "${AZURE_OPENAI_API_KEY}"

  evaluations:
    # === 标准对话质量评估 ===
    conversation_quality_test:
      models:
        - "openai-gpt-3.5-turbo"
        - "huggingface-microsoft-dialogpt-medium"
        # - "local-llama2-7b-chat"
      evaluators:
        - "chat"
        - "performance"
      maxConcurrency: 3
      saveResults: true
      outputPath: "results/conversation_quality"

      parameters:
        dataset_path: "datasets/conversation_qa.jsonl"
        dataset_limit: 100
        max_examples: 50       # 最大测试样本数
        timeout_seconds: 15    # 单个请求超时
        warmup_iterations: 5   # 预热迭代
        test_iterations: 30    # 正式测试迭代
        concurrent_levels: [1, 2, 5, 8]  # 并发压力测试级别

        # 评估标准
        similarity_threshold: 0.7
        include_latency: true
        include_accuracy: true

    # === 模型性能基准测试 ===
    performance_benchmark:
      models:
        - "openai-gpt-3.5-turbo"
      evaluators:
        - "performance"
      maxConcurrency: 5
      saveResults: true
      outputPath: "results/performance"

      parameters:
        max_examples: 200
        rounds: 10              # 测试轮数
        concurrent: 5           # 并发数
        number: 30             # 每轮请求数

        # 性能基准指标
        warmup_iterations: 10
        test_iterations: 50
        concurrent_levels: [1, 2, 5, 10, 15]
        load_patterns: ["steady", "burst"]   # 负载模式：稳定模式 vs 突发模式

    # === 批量对比评估 ===
    batch_comparison:
      models:
        - "openai-gpt-3.5-turbo"
        - "huggingface-microsoft-dialogpt-medium"
      evaluators:
        - "chat"
        - "performance"
      maxConcurrency: 5
      saveResults: true
      outputPath: "results/batch_comparison"

      parameters:
        max_examples: 150
        timeout_seconds: 30
        concurrent_levels: [1, 3, 8]
        metrics: "all"    # 收集所有指标: latency, accuracy, similarity

    # === 压力测试 ===
    stress_test:
      models:
        - "openai-gpt-3.5-turbo"
      evaluators:
        - "performance"
      maxConcurrency: 15
      saveResults: true
      outputPath: "results/stress_test"

      parameters:
        max_examples: 500
        timeouts_seconds: 25
        warmup_iterations: 3
        test_iterations: 100
        load_pattern: "burst"  # burst压力测试模式
        max_concurrent: 15

    # === API稳定性测试 ===
    api_stability_test:
      models:
        - "openai-gpt-3.5-turbo"
        - "huggingface-microsoft-dialogpt-medium"
      evaluators:
        - "performance"
      maxConcurrency: 2
      saveResults: true
      outputPath: "results/stability"

      parameters:
        max_examples: 300
        test_duration_minutes: 30     # 持续30分钟进行稳定性测试
        send_interval_ms: 1000        # 每秒一个请求，保持稳定负载
        max_retries_per_request: 5    # 单个请求最大重试次数

  settings:
    # 全局设置
    max_job_concurrency: 10          # 最大并发任务数
    response_timeout_seconds: 60     # 全局响应超时时间
    result_format: "json"            # 结果格式
    log_level: "INFO"                # 日志级别

    # 结果保存设置
    save_detailed_results: true
    save_raw_responses: true
    compress_results: true

    # 错误恢复设置
    fail_fast: false                # 遇到错误不立即停止
    max_fail_rate: 0.1             # 最大失败比例
    retry_on_error: true            # 是否在出错时重试

    # 性能监控设置
    enable_metrics_collection: true
    enable_memory_monitoring: false  # 是否监控内存使用
    enable_cpu_monitoring: false     # 是否监控CPU使用

# 环境变量说明:
# - OPENAI_API_KEY: OpenAI的API密钥 (必需)
# - HF_API_TOKEN: HuggingFace的API令牌 (可选，某些模型需要)
# - AZURE_OPENAI_API_KEY: Azure OpenAI服务的API密钥 (如果使用Azure)
#
# 使用方法:
# 1. 设置环境变量: export OPENAI_API_KEY='your-key'
# 2. 运行评估: java -jar evalscope-java.jar
# 3. 查看结果: 在 results/ 目录下查看详细报告

# 模型提供商支持:
# - openai: OpenAI官方API
# - azure-openai: Azure OpenAI服务
# - huggingface: HuggingFace Inference API
# - local: 本地部署的模型（兼容OpenAI格式）
# 可以通过 ModelFactory.registerCustomProvider() 注册自定义提供商