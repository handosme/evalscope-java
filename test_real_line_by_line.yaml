evalscope:
  evaluations:
    real_line_by_line_test:
      evaluationName: "real_line_by_line_test"
      modelIds: ["mock-model"]
      evaluatorTypes: ["performance"]
      datasetPath: "test_prompts.txt"
      maxConcurrency: 1
      saveResults: true
      resultFormat: "json"
      outputPath: "results/real_line_by_line_test/"
      parameters:
        dataset: "line_by_line"
        max_examples: 15
        concurrent: 1
        warmup_iterations: 2
        test_iterations: 8
        shuffle: false

  models:
    mock-model:
      modelType: "chat"
      provider: "mock"
      enabled: true
      parameters:
        response_delay_ms: 25